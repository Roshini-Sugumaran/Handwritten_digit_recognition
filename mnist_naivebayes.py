# -*- coding: utf-8 -*-
"""MNIST_Naivebayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1McxM5RnjZau6G-LUI5MbZEp-CMwfwzVl
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow

from tensorflow.keras.datasets import mnist

# Load MNIST dataset
(train_x, train_y), (test_x, test_y) = mnist.load_data()

print(f'Training data shape: {train_x.shape}, Training labels shape: {train_y.shape}')
print(f'Testing data shape: {test_x.shape}, Testing labels shape: {test_y.shape}')

import matplotlib.pyplot as plt
# Function to visualize a grid of images
def visualize_mnist_data(images, labels, num_images=16):
    plt.figure(figsize=(10, 10))
    for i in range(num_images):
        # Define a subplot in a grid of 4x4
        plt.subplot(4, 4, i + 1)
        plt.imshow(images[i], cmap='gray')
        plt.title(f'Label: {labels[i]}')
        plt.axis('off')
    plt.show()

# Visualize the first 16 images from the training set
visualize_mnist_data(train_x, train_y, num_images=16)

train_x = train_x.reshape(60000, 784)
test_x = test_x.reshape(10000, 784)
print(f'Training data shape: {train_x.shape}, Training labels shape: {train_y.shape}')
print(f'Testing data shape: {test_x.shape}, Testing labels shape: {test_y.shape}')

import numpy as np
# Combine train and test data into a single array and shuffle it randomly
total_data = np.zeros((70000, 785))
total_data[:60000, :-1] = train_x
total_data[60000:, :-1] = test_x
total_data[:60000, -1] = train_y
total_data[60000:, -1] = test_y

print(f' Total data shape: {total_data.shape}')

np.random.shuffle(total_data)

data = total_data[:, :-1]
label = total_data[:, -1]

import numpy as np

def train_naivebayes(data, label, train_ratio=0.85):
    n_s, n_f = data.shape
    classes = np.unique(label)
    n_c = len(classes)

    # Initialize arrays for mean and variance of each class
    mean_X = np.zeros((n_c, n_f))
    variance_X = np.zeros((n_c, n_f))
    Proc_c = np.zeros(n_c)  # Prior probabilities

    # Split data into training and testing sets
    split_idx = int(train_ratio * n_s)
    TrainData = data[:split_idx]
    TrainLabels = label[:split_idx]

    # Calculate mean, variance, and class probabilities (priors)
    for idx, c in enumerate(classes):
        trainx_c = TrainData[TrainLabels == c]
        Proc_c[idx] = len(trainx_c) / len(TrainData)
        mean_X[idx, :] = trainx_c.mean(axis=0)
        variance_X[idx, :] = trainx_c.var(axis=0) + 1e-5  # Variance with added constant to prevent division by zero

    Proc_c = np.log(Proc_c)  # Log of priors for stability

    return mean_X, variance_X, Proc_c, classes, TrainData, TrainLabels

import numpy as np
from sklearn.metrics import confusion_matrix

def test_naivebayes(mean_X, variance_X, Proc_c, classes, TestData, TestLabels):
    n_c = len(classes)
    n_f = mean_X.shape[1]

    # Naive Bayes classification on test data with vectorized calculations
    predictedY = np.zeros(len(TestLabels), dtype=int)

    # Precompute constants
    log_sqrt_2pi_var = np.log(np.sqrt(2 * np.pi * variance_X))

    # For each test sample
    for i in range(len(TestData)):
        # Compute log probabilities for each class in a vectorized manner
        log_likelihoods = np.zeros(n_c)

        for j in range(n_c):
            diff = TestData[i] - mean_X[j]
            log_likelihoods[j] = Proc_c[j] + np.sum(-0.5 * (diff ** 2 / variance_X[j]) - log_sqrt_2pi_var[j])

        # Pick the class with the highest log posterior probability
        predictedY[i] = np.argmax(log_likelihoods)

    confusionMatrix = confusion_matrix(TestLabels, predictedY, labels=classes)

    # Calculate overall accuracy
    totCorrectPrict = np.sum(TestLabels == predictedY)
    OverallAccuracy = totCorrectPrict / len(TestLabels)

    return OverallAccuracy, confusionMatrix, predictedY,TestLabels

# Assume `data` and `label` are already defined
mean_X, variance_X, Proc_c, classes, TrainData, TrainLabels = train_naivebayes(data, label)
split_idx = int(0.85 * data.shape[0])
TestData = data[split_idx:]
TestLabels = label[split_idx:]

OverallAccuracy, confusionMatrix, predictedY,TestLabels = test_naivebayes(mean_X, variance_X, Proc_c, classes, TestData, TestLabels)

print("Overall Accuracy:", OverallAccuracy)
print("Confusion Matrix:\n", confusionMatrix)

# Print the shapes of TestLabels and predictedY
print("Shape of TestLabels:", TestLabels.shape)
print("Shape of predictedY:", predictedY.shape)
# Print the first 10 elements of TestLabels and predictedY
num_samples_to_print = 10

print("First 10 TestLabels:")
print(TestLabels[:num_samples_to_print])

print("First 10 predictedY:")
print(predictedY[:num_samples_to_print])

import matplotlib.pyplot as plt
import numpy as np

def visualize_misclassified_images_flattened(TestData, TestLabels, predictedY, classes, image_shape=(28, 28), num_images=10):
    # Reshape TestData to its original shape
    reshaped_TestData = TestData.reshape(-1, *image_shape)

    # Ensure TestLabels and predictedY are integers
    TestLabels = np.array(TestLabels, dtype=int)
    predictedY = np.array(predictedY, dtype=int)

    # Get indices of misclassified images
    misclassified_indices = np.where(TestLabels != predictedY)[0]

    # If there are no misclassified instances, return
    if len(misclassified_indices) == 0:
        print("No misclassified images found.")
        return

    # Determine number of images to plot
    num_misclassified = min(num_images, len(misclassified_indices))

    # Create a plot with subplots
    fig, axes = plt.subplots(1, num_misclassified, figsize=(15, 5))

    # Ensure axes is iterable
    if num_misclassified == 1:
        axes = [axes]

    for i in range(num_misclassified):
        idx = misclassified_indices[i]
        true_label = TestLabels[idx]
        predicted_label = predictedY[idx]

        # Plot image
        ax = axes[i]
        ax.imshow(reshaped_TestData[idx], cmap='gray')  # Use 'gray' for grayscale images
        ax.set_title(f"True: {classes[true_label]}\nPred: {classes[predicted_label]}")
        ax.axis('off')

    plt.show()

# Example usage
# Ensure `TestData`, `TestLabels`, `predictedY`, and `classes` are defined correctly
# Example: classes = [str(i) for i in range(10)]  # For MNIST dataset
# visualize_misclassified_images_flattened(TestData, TestLabels, predictedY, classes)

visualize_misclassified_images_flattened(TestData, TestLabels, predictedY, classes)